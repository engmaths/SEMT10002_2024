{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e958b01-4b3a-47f4-9b16-afd4f43d0120",
   "metadata": {},
   "source": [
    "<h1> kMeans and k Nearest Neighbours</h1>\n",
    "\n",
    "This week, we are going to learn about and implement two loosely related machine-learning algorithms. In this lab, we'll help you to implement the *kMeans* algorithm, and then for your consolidation exercise, you'll work in pairs to implement *k Nearest Neighbours* (kNN). In both cases, you should try to not only write code that is correct, but also ensure your code is readable. The best way to do this is to stick to the unit style guide (available on Blackboard).\n",
    "\n",
    "<h2> kMeans</h2>\n",
    "\n",
    "Before we dive in to code, we need to introduce some background ideas so we can understand what kMeans is trying to do and why we'd want to use it. kMeans is an *unsupervised* *clustering* algorithm. A *clustering* algorithm tries to organise some data points into groups (or *clusters* based on the values of the data points. \n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/cluster_example.png?raw=true\" width=\"80%\">\n",
    "\n",
    "A *supervised* clustering algorithm would do this by training a model on data which contains the *true* cluster allocations. For example, you may want to use a clustering algorithm to identify which students are most likely to fail a unit based on their engagement with formative assessments. To do this, we'd need some data consisting of the student's engagement record and whether they passed or failed the unit. In many cases, however, we don't have access to (or simply don't know) the true allocations. In these cases, we have to use an unsupervised algorithm, which tries to infer the allocations based on other properties of the data. kMeans is an example of such an algorithm. \n",
    "\n",
    "<h3> How it works</h3>\n",
    "\n",
    "The basic idea behind kMeans is to try to assign a data point to the cluster which it is *closest* too. It does this by calculating the mean (hence kMeans) of all data points assigned to a cluster. Once the means have been calculated, the algorithm loops through each data point, calculating the distance to each cluster mean and re-assigning points to the cluster it is closest too. \n",
    "\n",
    "The algorithm described in the paragraph above suffers from a 'chicken and egg' problem. In order to calculate the mean of a cluster, we need to have assigned each of our data points to a cluster already. But we can't assign a point to a cluster if we don't know where the mean is-- so how do we get started? \n",
    "\n",
    "Well, it turns out that we can start with *random* means for our clusters and then repeat a process of first assigning points to the (random) clusters and then re-calculating the mean points. Over time, we'll see that often (but not always!), the number of points which change cluster reduces, until eventually the algorithm converges (i.e. we do an update and no points are re-assigned to different clusters). Convergence isn't guaranteed with kMeans, so it's a good idea to set an upper limit on the number of iterations we want to run.\n",
    "\n",
    "As a sequence of steps, we can write this down as:\n",
    "\n",
    "1. Decide how many clusters we want (i.e. what value of K should we use). \n",
    "2. Randomly initialise K cluster means\n",
    "3. Repeat until convergence\n",
    "   \n",
    "    a. Assign each data point to the cluster it is closest to\n",
    "   \n",
    "    b. Re-calculate the mean position of each cluster.\n",
    "\n",
    "To calculate distance, kMeans use the *Euclidean distance* metric. If you haven't heard of this before, the idea is that if we have two points we can draw a triangle between them, formed by the hypotenuse (i.e. the distance between the points), the horizontal separation, and the vertical separation. The horizontal ($x_2 - x_1$) and vertical ($y_2 - y_1$) separations can be calculated directly from our data points. The distance can then be calculated using Pythagoras theorem- $d = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/euclidean_distance.png?raw=true\" width=\"80%\">\n",
    "\n",
    "This is the situation for when we have 2D data- if our data was 3D, then we would simply modify the equation to include an additional *separation* term (i.e $z_2-z_1)$. In fact, the same equation can work for any number of dimensions. This is good, because data points in machine learning are often very highly dimensional!\n",
    "\n",
    "Let's see what happens when we run kMeans on the data plotted above. We start by choosing 3 random data points to be our initial cluster means (shown by the '+' in our plot).\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step1.png?raw=true\" width=\"80%\">\n",
    "\n",
    "Next, we assign each data point to the cluster it is closest to.\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step2.png?raw=true\" width=\"80%\">\n",
    "\n",
    "Next, we update our cluster means\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step3.png?raw=true\" width=\"80%\">\n",
    "\n",
    "Then we re-assign each data point to the closest cluster\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step4.png?raw=true\" width=\"80%\">\n",
    "\n",
    "and again update our cluster means\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step5.png?raw=true\" width=\"80%\">\n",
    "\n",
    "After two more iterations we converge on a solution\n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step6.png?raw=true\" width=\"80%\">\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step7.png?raw=true\" width=\"80%\">\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/step8.png?raw=true\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56a769-2e20-4ad1-a0cb-5e49959d78e0",
   "metadata": {},
   "source": [
    "<h3> Implementing kMeans</h3>\n",
    "\n",
    "Before we can start implementing kMeans, we need a data set to apply it to. The file \"heart_data.csv\" contains data from 297 patients who suffered from heart failure. We'd like to run kMeans on this data to see how many different underlying causes led to heart failure. Our data set contains 13 bits of information (or *feature* as we normally call them in machine learning) about each patient. We'll begin by just looking at two- the patient's age and the results of a test for the presence of creatanine phosphokinase. \n",
    "\n",
    "**Exercise 1**\n",
    "\n",
    "Download the file \"heart_data.csv\" and write some code for reading the contents of the file. You should store the data as a single numpy array of size (2, 297). Then create a scatter plot of age against creatanine phosphokinase. Your plot should look like the image below. \n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/heart_data_plot.png?raw=true\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebff6bf-4d8f-4ea7-a994-233364bf74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "file = \"heart_data.csv\"\n",
    "\n",
    "#Your code goes here\n",
    "\n",
    "#Store the data as a 2 x 297 numpy array.\n",
    "data = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c4b8b-6e13-4aa6-bb71-99cf9f2b8744",
   "metadata": {},
   "source": [
    "Now that we've got some data to process, we can start working on our function for implementing kMeans. At the highest level, this code should will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cb8d36-dc76-4593-8c1b-705724d3a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(k, data, max_iterations = 100):\n",
    "\n",
    "    #We need to randomly initialise the means (or centroids) before we can start\n",
    "    centroids = initalise_centroids(k, data)\n",
    "\n",
    "    #kMeans doesn't always converge, so we set a maximum number of iterations to stop our code from running forever. \n",
    "    for iteration in range(max_iterations):\n",
    "        #We start by assigning data to clusters\n",
    "        clusters = assign_data_to_clusters(centroids, data)\n",
    "        #Next we update our centroids\n",
    "        centroids = calculate_cluster_means()\n",
    "\n",
    "    #Once finished, let's plot the clusters\n",
    "    plot_clusters(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfcca7-51d8-4b44-b3b0-e398a7a23199",
   "metadata": {},
   "source": [
    "Here, we've got four functions that we need to define in order for this code to work. Let's implement each in turn.\n",
    "\n",
    "<h4>Initialising centroids</h4>\n",
    "\n",
    "We need to randomly choose some starting centroid means. A simple way to do this is to  randomly select K points and use their position at the cluster means. We can do this by randomly generating K integers between 0 and the size of our data set and using these to index into the data array.\n",
    "\n",
    "**Exercise 2**\n",
    "Complete the function below to randomly initalise the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99daa39d-8aa6-400c-afbc-8fabfcde4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_centroids(k, data):\n",
    "\n",
    "    #Centroids should be a list of k points, where each point is a tuple (feature1, feature2).\n",
    "    centroids = []\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ad0b1f-25fe-41a4-88e3-cea365f83a14",
   "metadata": {},
   "source": [
    "<h4> Assigning Data to Clusters</h4>\n",
    "\n",
    "To assign a data point to a cluster, we need to find which cluster mean it is closest too. The obvious thing to do here is to loop through the data, and for each point calculate the distance to each centroid. We then add the data point to the cluster it is closest too. \n",
    "\n",
    "<h4> Calculating cluster means </h4>\n",
    "\n",
    "To calculate the mean position of each cluster, we need to calculate the mean of each feature for every point in the cluster. In other words, the mean position of cluster 1 is given by (mean_of_cluster_1_first_feature, mean_of_cluster_1_second_feature, ...). We can do this by creating a slice for each feature of the data in cluster 1, calculating the mean and then creating a new tuple to store the position. \n",
    "\n",
    "<h4> Plotting</h4>\n",
    "\n",
    "To plot the data, we can use a matplotlib scatter plot. The only complication is that we would like to plot each cluster in a different color. You can do this by using the 'color' keyword when you call the scatter function. To plot each cluster, we need to loop over all clusters, for each one plotting the data points it contains with a different color.\n",
    "\n",
    "**Exercise 3**\n",
    "Implement the three functions described above. You can do this with loops and list comprehensions, but I'd strongly encourage you to think about the *NumPy vectorised* operations we saw in last week's lab. Once you've implemented all three functions, your kMeans implementation is finished- try running it on the data set and see what happens.  Note that because of the random initialisation, you will get slightly different results each time.  What happens as you vary the value of K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e1dde0-ab7e-4c8d-9c9e-8f21bbe21b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_data_to_clusters(centroids, data):\n",
    "    #Your code goes here\n",
    "\n",
    "    #Clusters should be a list of numpy arrays. Each array should contain all data points assigned to a cluster\n",
    "    clusters = [np.array([]) for val in len(centroids)]\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def calculate_cluster_means(clusters):\n",
    "\n",
    "    #Centroids should be a list of points (x, y, ... ), with each point representing the mean position of all data in a cluster\n",
    "    centroids = [0 for val in len(clusters)]\n",
    "\n",
    "    return centroids\n",
    "\n",
    "def plot_clusters(clusters):\n",
    "\n",
    "    #Your code goes here. You should plot all data, using a different color to represent each cluster. \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08f37c-e25c-400c-9ba8-5c73d959ac7c",
   "metadata": {},
   "source": [
    "<h3> Testing k Means</h3>\n",
    "\n",
    "We haven't provided you with any test functions to check whether your code is correct or not- think about how you can confirm whether your code is doing the right thing or not. \n",
    "\n",
    "**Exercise 4** Add test functions for your code. You should think carefully about which functions need testing and how to test them. How could you test the overall functionality of the code?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b87936ab-26a9-47ba-936e-0d2b716afc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kmeans():\n",
    "    pass\n",
    "    #Your code should go here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bedb3-d265-4cb8-8a1e-26d6819dae8d",
   "metadata": {},
   "source": [
    "<h3> Scikit-Learn</h3>\n",
    "\n",
    "**Bonus Exercise 1**\n",
    "\n",
    "kMeans is a commonly used machine learning algorithm. Although we've implemented it ourselves here, there are many libraries which contain their own (optimised) implementations, and conventionally we'd use one of those libraries rather than writing it ourselves. One such library is scikit-learn, a popular machine learnng library for Python. The documentation for their implementation of kMeans is <a href=https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>here</a>. Try installing (with pip3) scikit-learn and running their version of kMeans. How do the results compare to our implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a101bf2-a06a-4db3-a78b-70d619ac2321",
   "metadata": {},
   "source": [
    "<h3> Identifying convergence</h3>\n",
    "\n",
    "**Bonus Exercise 2**\n",
    "\n",
    "Our implementation of kMeans currently runs for a fixed number of steps (here, 100). However, in many cases, kMeans will converge far quicker than this- in other words, we can get a result quicker if we can stop when the algorithm has converged. \n",
    "\n",
    "Here, we define convergence as when we run an iteration of kMeans and no data points are re-assigned to different clusters. \n",
    "\n",
    "Implement the function is_converged(old_clusters, new_clusters) which checks for convergence. Once complete, you'll need to modify the kMeans function to stop early if convergence is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a39c87-47a1-4b9c-9ef1-2e05add7ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_converged(old_clusters, new_clusters):\n",
    "    #Your code goes here.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bc799-753e-49e1-8093-61baafc40ca3",
   "metadata": {},
   "source": [
    "<h2> k Nearest Neighbours</h2>\n",
    "\n",
    "k Nearest Neighbours (kNN) is a *supervised* *classification* algorithm. A classification algorithm tries to identify which of N classes a data point belongs to. For example, given some data on a student's performance in their year 1 assessments, we might want to classify them into those likely to drop out and those likely to complete the course. \n",
    "\n",
    "kNN is a *supervised* algorithm, so has access to some data (typically called *training data*) which contains both our *features* and the *true classes*. Given a new data point (typically called *test data*), it then predicts a class for the new point based on the new data point's features. kNN uses a simple heuristic to do this- use the modal value of the k closest points in the data set. \n",
    "\n",
    "<img src=\"https://github.com/engmaths/SEMT10002_2024/blob/main/img/KnnClassification.svg?raw=true\" width=\"40%\">\n",
    "\n",
    "In the image above, the green circle represents a test point. If K=3, then the neighbours would be two red triangles and a blue square, so we would predict that the test point is a red triangle. If however, we used K=5, then we'd have two additional blue squares, so we'd predict that the test point is a blue square.\n",
    "\n",
    "<h3> Implementing kNN</h3>\n",
    "\n",
    "The algorithm for implementing kNN is relatively short and sweet. Given a test point, we calculate the Euclidean distance to every point in our training data. We then find the K closest points, returning the  modal value. \n",
    "\n",
    "**Exercise 5**\n",
    "\n",
    "The file heart_data.csv (i.e. the same data as we used for kMeans) also contains a feature that records whether a patient survived or not (\"DEATH_EVENT\" in the file). For this exercise, you need to implement kNN and use it to predict whether a patient is likely to survive or not. You should base your decision on three features of the data set- *age*, *creatinine_phosphokinase*, and *ejection_fraction*. The data file contains 297 patient records- you should use the first 250 records as your training data and the final 47 as the *test* data. \n",
    "\n",
    "Once you have implemented kNN, I'd like you to explore which value of k produces the best predictions. To do this, you should call your function with values of K in the range 1-10, calculating the mean squared error for each value of K. As a reminder, the mean squared error is defined by the formula:\n",
    "\n",
    "$ MSE = \\frac{1}{n} \\sum_{i=1}^n(Y_{predicted, i} - Y_{actual, i})^2$\n",
    "\n",
    "where $Y_{predicted, i}$ is your prediction for the ith test point and $Y_{actual, i} is the actual value for the ith test point (i.e. what the file heart_data.csv actually contains). \n",
    "\n",
    "**Submission** This code will be submitted (in pairs) as your fourth consolidation exercise. Both pairs should upload identical code to Blackboard by February 7th. Your code should be submitted as a single file (\"kNN.py\"). When I run your code, it should create a plot of the mean-squared-error for different values of K (see image below for example). \n",
    "\n",
    "Feedback for this assignment will focus on your use of NumPy, your use of functions to organise code, and your adherence to the unit style guide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9314a4b-bafa-4026-932b-3c211ee658a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
